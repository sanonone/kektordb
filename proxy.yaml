port: ":9092"
target_url: "http://localhost:11434"

# Configurazione Embedder (usa il tuo modello locale)
embedder_type: "ollama_api"
embedder_url: "http://localhost:11434/api/embeddings"
embedder_model: "nomic-embed-text-v2-moe:latest"
embedder_timeout: 60s

# RAG Automatico
rag_enabled: true
rag_index: "knowledge_base"  # Deve coincidere con l'indice popolato dai Vectorizer!
rag_top_k: 8
# RAG usa SIMILARITÀ (Higher is Better, 0.0-1.0)
rag_threshold: 0.7         # Ignore chunks with similarity score < 0.6
rag_use_hybrid: false # Cerca anche per parole chiave
rag_hybrid_alpha: 0.9      # 70% Vettore, 30% Keywords
rag_use_graph: true        # Recupera automaticamente contesto prev/next
rag_use_hyde: true
# System Prompt Template
# Usa {{context}} per iniettare i dati trovati e {{query}} per la domanda originale.
# Il simbolo '|' permette stringhe multilinea in YAML.


# Firewall (Prompt Guard)
firewall_enabled: true
firewall_index: "prompt_guard"
# Firewall usa DISTANZA (Lower is Better)
firewall_threshold: 0.25 # Block if distance < 0.25
block_message: "I cannot fulfill this request as it violates safety policies."

# Semantic Cache
cache_enabled: false
cache_index: "semantic_cache_test"
# Cache usano DISTANZA (Lower is Better)
cache_threshold: 0.1 # Cache hit if distance < 0.1
cache_ttl: "5s"        
max_cache_items: 100

cache_vacuum_interval: "2s" 
cache_delete_threshold: 0.0 # 0.0 = Pulisci sempre se c'è anche solo 1 cancellazione

# rewriter
fast_llm:
  base_url: "http://localhost:11434/v1"
  model: "gemma3:4b"

# HyDe
llm:
  base_url: "http://localhost:11434/v1"
  model: "gemma3:4b"
