port: ":9092"
target_url: "http://localhost:11434"

# Configurazione Embedder (usa il tuo modello locale)
embedder_type: "ollama_api"
embedder_url: "http://localhost:11434/api/embeddings"
embedder_model: "nomic-embed-text:latest"
embedder_timeout: 60s

# RAG Automatico
rag_enabled: true
rag_index: "knowledge_base"  # Deve coincidere con l'indice popolato dai Vectorizer!
rag_top_k: 6
rag_threshold: 0.4           # Se la distanza > 0.4 (poco simile), non inietta nulla
rag_use_hybrid: false       # Cerca anche per parole chiave
rag_hybrid_alpha: 0.7      # 70% Vettore, 30% Keywords
rag_use_graph: true        # Recupera automaticamente contesto prev/next
# System Prompt Template
# Usa {{context}} per iniettare i dati trovati e {{query}} per la domanda originale.
# Il simbolo '|' permette stringhe multilinea in YAML.
rag_system_prompt: |
  You are a helpful and precise assistant.
  Use ONLY the following context to answer the user's question.
  If the answer is not in the context, say "I don't have enough information in my knowledge base".
  Do not make up facts.

  CONTEXT:
  {{context}}

  QUESTION:
  {{query}}

# Firewall (Prompt Guard)
firewall_enabled: true
firewall_index: "prompt_guard"
firewall_threshold: 0.25 # Blocca se molto simile
block_message: "I cannot fulfill this request as it violates safety policies."

# Semantic Cache
cache_enabled: true
cache_index: "semantic_cache_test"
cache_threshold: 0.1
cache_ttl: "5s"        
max_cache_items: 100

cache_vacuum_interval: "2s" 
cache_delete_threshold: 0.0 # 0.0 = Pulisci sempre se c'Ã¨ anche solo 1 cancellazione
